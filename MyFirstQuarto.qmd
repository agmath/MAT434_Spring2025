---
title: My First Quarto Document
author: 
  - name: Adam Gilbert
    email: a.gilbert1@snhu.edu
    affiliations: 
      - name: Southern New Hampshire University
format: html
date: 9/15/2024
date-modified: today
date-format: long
theme: flatly
toc: true
---

## Working with Data

### Loading Data

```{r}
#| message: false
#| code-fold: true

library(tidyverse)
library(tidymodels)
library(skimr)

hits <- read_csv("https://raw.githubusercontent.com/agmath/agmath.github.io/refs/heads/master/data/classification/battedballs.csv")
parks <- read_csv("https://raw.githubusercontent.com/agmath/agmath.github.io/refs/heads/master/data/classification/park_dimensions.csv")

theme_set(theme_bw(base_size = 14))
```

## Exploring Our Data

```{r}
#head(hits)

hits %>% 
  head()
```

```{r}
parks %>%
  head()
```

### Joining the Data

```{r}
hits <- hits %>%
  left_join(parks, by = c("park"  = "park"))
```

We joined the `hits` and `parks` data together, to obtain a full data set with `r nrow(hits)` rows and `r ncol(hits)` columns.

### Initial explorations

View the first 6 rows of observations

```{r}
hits %>%
  head()
```

See the "type" (according to R) of each variable in the data set

```{r}
hits %>%
  glimpse()
```

Skimming the data set to get a surface level analysis. This is useful for identifying data issues, like missing values or for identifying extreme class imbalances. For example, only abotu 5.5% of observations resulted in home runs while the remaining 94.5% were not home runs.

```{r}
hits %>%
  skim()
```

### Split into training and test data

Remember that we need some data for us (and our eventual models) to *learn* from and then another set of observations to assess our model performance against. The code below splits our data into a *training* pile (the data we can use and learn from) and a *testing* pile (the data we'll hide from ourselves and our models until the very end of our project). The `set.seed()` function ensures that we obtain the same *training* and *test* data every time we run this notebook. Be sure to set a seed any time you are splitting your data (or doing anything else that involves randomness).

```{r}
set.seed(434)
data_splits <- initial_split(hits, 0.85, strata = is_home_run)

train <- training(data_splits)
test <- testing(data_splits)
```

## Exploratory Data Analysis

We want to know how and why home runs happen. Can we predict what types of scenarios are most likely to end in a home run? 

### Sometimes Useful Functionality

The following functions: `filter()` and `select()` are not directly useful here. They allow us to view, or work with, a subset of our available data. The `filter()` function returns only rows satisfying the criteria we dictate, while the `select()` function returns only the columns we request.

Filtering rows

```{r}
train %>%
  filter(is_home_run == 1)

train %>%
  filter(launch_angle > 45)

train %>%
  filter(str_detect(NAME, "Fenway"))
```

Selecting just a few columns

```{r}
train %>%
  select(launch_speed, launch_angle, is_home_run)
```

### Feature Engineering (Creating New Variables)

Building new variables from old ones (*Feature Engineering*) can be done with the `mutate()` function. This function either adds a new column to your data set or edits an existing column. Inside of `mutate()` the text to the left of the equal sign (`=`) is the column name and the expression to the right of the equal sign (`=`) indicates how the values under this column will be computed.

```{r}
train %>%
  mutate(fast_pitch = ifelse(pitch_mph > 100, "yes", "no"))
```

Note that the new variable is only added temporarily. We'd need to store the result of the mutation in order to retain access to the new variable.

```{r}
train_with_fast_pitch <- train %>%
  mutate(fast_pitch = ifelse(pitch_mph > 100, "yes", "no"))
```

### Summary statistics

Home runs...

```{r}
train %>%
  count(is_home_run) %>%
  mutate(prop = 100*n/sum(n))
```

```{r}
train %>%
  summarize(pct_hr = 100*mean(is_home_run))
```

Summarizing Launch Angle...

```{r}
train %>%
  filter(!is.na(launch_angle)) %>%
  summarize(
    min_angle = min(launch_angle),
    mean_angle = mean(launch_angle),
    median_angle = median(launch_angle),
    max_angle = max(launch_angle),
    sd_angle = sd(launch_angle)
  )
```

#### Grouped Summaries

```{r}
train %>%
  group_by(NAME, is_home_run) %>%
  filter(!is.na(launch_angle)) %>%
  summarize(
    min_angle = min(launch_angle),
    mean_angle = mean(launch_angle),
    median_angle = median(launch_angle),
    max_angle = max(launch_angle),
    sd_angle = sd(launch_angle)
  )
```

### Data Viz

```{r}
train %>%
  ggplot() + 
  geom_histogram(aes(x = pitch_mph)) + 
  labs(
    title = "Pitch Speeds",
    x = "Pitch Speed (mph)",
    y = ""
  )
```

Without ChatGPT help...

```{r}
train %>%
  ggplot() + 
  geom_histogram(aes(x = pitch_mph, y = after_stat(density))) +
  geom_density(aes(x = pitch_mph))
```

With ChatGPT enhancements...

```{r}
train %>%
  filter(pitch_name != "Forkball") %>%
  ggplot(aes(x = pitch_mph)) + 
  geom_histogram(aes(y = after_stat(density)), binwidth = 2, fill = "#377EB8", color = "black", alpha = 0.7) +
  geom_density(aes(color = pitch_name, fill = pitch_name), alpha = 0.4, size = 1) +
  facet_wrap(~ pitch_name, scales = "free_y") +
  labs(
    title = "Pitch Speeds by Pitch Type",
    x = "Pitch Speed (mph)",
    y = "Density",
    caption = "Data Source: Baseball Dataset"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5),
    strip.text = element_text(size = 12, face = "bold"),
    legend.position = "none"
  )
```

A plot on pitch locations and home runs:

```{r}
#| fig.height: 12

train %>%
  mutate(is_home_run = ifelse(is_home_run == 1, "HomeRun", "Not"),
         is_batter_lefty = ifelse(is_batter_lefty, "BatsLeft", "BatsRight"),
         is_pitcher_lefty = ifelse(is_pitcher_lefty, "PitcherLeft", "PitcherRight")) %>%
  filter(pitch_name != "Forkball") %>%
  ggplot() + 
  geom_point(aes(x = plate_x, y = plate_z, color = is_home_run, alpha = ifelse(is_home_run == "HomeRun", 0.8, 0.25))) +
  facet_grid(pitch_name ~ is_batter_lefty) +
  labs(
    title = "Pitch Locations, Types, and HomeRuns",
    x = "Plate Horizontal",
    y = "Plate Vertical",
    color = "Hit Type"
  ) + 
  scale_color_manual(values = c("purple", "darkgreen")) +
  theme(legend.position = "bottom") + 
  guides(alpha = "none")
```

## Model Construction, Evaluation, and Tuning

Prep our data...

```{r}
hits_for_model <- hits %>%
  mutate(is_home_run = ifelse(is_home_run == 1, "yes", "no")) %>%
  mutate(is_home_run = factor(is_home_run, levels = c("no", "yes")))

set.seed(434)
data_splits <- initial_split(hits_for_model, 0.85, strata = is_home_run)

train <- training(data_splits)
test <- testing(data_splits)
```

Fitting a model...

```{r}
dt_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

dt_rec <- recipe(is_home_run ~ launch_speed + launch_angle + pitch_mph + pitch_name + Cover, data = train) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

dt_wf <- workflow() %>%
  add_model(dt_spec) %>%
  add_recipe(dt_rec)

dt_fit <- dt_wf %>%
  fit(train)
```

Let's see the model...

```{r}
dt_fit %>%
  extract_fit_engine() %>%
  rpart.plot::rpart.plot()
```

Assess our model's performance...

Training data...

```{r}
dt_fit %>%
  augment(train) %>%
  accuracy(is_home_run, .pred_class)
```

Testing data...

```{r}
dt_fit %>%
  augment(test) %>%
  accuracy(is_home_run, .pred_class)
```

Cross-Validation

```{r}
train_folds <- vfold_cv(train, v = 10, strata = is_home_run)

dt_cv_results <- dt_wf %>%
  fit_resamples(train_folds)

dt_cv_results %>%
  collect_metrics()
```

Tuning our model...

```{r}
dt_spec <- decision_tree(tree_depth = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")

dt_wf <- workflow() %>%
  add_model(dt_spec) %>%
  add_recipe(dt_rec)

dt_tune_results <- dt_wf %>%
  tune_grid(
    resamples = train_folds,
    grid = 10
  )

dt_tune_results %>%
  show_best(n = 10, metric = "accuracy")
```










